{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python trainer.py --db sleepedf --gpu 0 --from_fold 0 --to_fold 19`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from data import load_data, get_subject_files\n",
    "from model import Model\n",
    "from minibatching import iterate_batch_multiple_seq_minibatches\n",
    "from utils import print_n_samples_each_class, load_seq_ids\n",
    "from logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = \"Namespace(db='sleepedf', from_fold=0, gpu=0, n_epochs=200, random_seed=42, suffix='', test_batch_size=15, test_seq_len=20, to_fold=19)\"\n",
    "db = \"sleepedf\"\n",
    "gpu = 0\n",
    "from_fold = 0\n",
    "to_fold = 19\n",
    "suffix = \"\"\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = os.path.join(\"config\", f\"{db}.py\")\n",
    "spec = importlib.util.spec_from_file_location(\"*\", config_file)\n",
    "config = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_idx = 0\n",
    "output_dir = \"./output/train\"\n",
    "log_file = \"./output/output.log\"\n",
    "restart = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug `def train()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = importlib.util.spec_from_file_location(\"*\", config_file)\n",
    "config = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(config)\n",
    "config = config.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for the specified fold_idx\n",
    "output_dir = os.path.join(output_dir, str(fold_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if restart:\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "else:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "# Create logger\n",
    "logger = get_logger(log_file, level=\"info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subject_files = glob.glob(os.path.join(config[\"data_dir\"], \"*.npz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INFO ]\u001b[0m Load generated SIDs from sleepedf.txt\n",
      "\u001b[36m[INFO ]\u001b[0m SIDs (20): [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load subject IDs\n",
    "fname = \"{}.txt\".format(config[\"dataset\"])\n",
    "seq_sids = load_seq_ids(fname)\n",
    "logger.info(\"Load generated SIDs from {}\".format(fname))\n",
    "logger.info(\"SIDs ({}): {}\".format(len(seq_sids), seq_sids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INFO ]\u001b[0m Train SIDs: (17) [ 2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "\u001b[36m[INFO ]\u001b[0m Valid SIDs: (2) [1 6]\n",
      "\u001b[36m[INFO ]\u001b[0m Test SIDs: (1) [0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split training and test sets\n",
    "fold_pids = np.array_split(seq_sids, config[\"n_folds\"])\n",
    "\n",
    "test_sids = fold_pids[fold_idx]\n",
    "train_sids = np.setdiff1d(seq_sids, test_sids)\n",
    "\n",
    "# Further split training set as validation set (10%)\n",
    "n_valids = round(len(train_sids) * 0.10)\n",
    "\n",
    "# Set random seed to control the randomness\n",
    "np.random.seed(random_seed)\n",
    "valid_sids = np.random.choice(train_sids, size=n_valids, replace=False)\n",
    "train_sids = np.setdiff1d(train_sids, valid_sids)\n",
    "\n",
    "logger.info(\"Train SIDs: ({}) {}\".format(len(train_sids), train_sids))\n",
    "logger.info(\"Valid SIDs: ({}) {}\".format(len(valid_sids), valid_sids))\n",
    "logger.info(\"Test SIDs: ({}) {}\".format(len(test_sids), test_sids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "for sid in train_sids:\n",
    "    train_files.append(get_subject_files(\n",
    "        dataset=config[\"dataset\"],\n",
    "        files=subject_files,\n",
    "        sid=sid,\n",
    "    ))\n",
    "train_files = np.hstack(train_files)\n",
    "train_x, train_y, _ = load_data(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_files = []\n",
    "for sid in valid_sids:\n",
    "    valid_files.append(get_subject_files(\n",
    "        dataset=config[\"dataset\"],\n",
    "        files=subject_files,\n",
    "        sid=sid,\n",
    "    ))\n",
    "valid_files = np.hstack(valid_files)\n",
    "valid_x, valid_y, _ = load_data(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = []\n",
    "for sid in test_sids:\n",
    "    test_files.append(get_subject_files(\n",
    "        dataset=config[\"dataset\"],\n",
    "        files=subject_files,\n",
    "        sid=sid,\n",
    "    ))\n",
    "test_files = np.hstack(test_files)\n",
    "test_x, test_y, _ = load_data(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INFO ]\u001b[0m Training set (n_night_sleeps=0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e314c6dde25d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training set (n_night_sleeps={})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint_n_samples_each_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation set (n_night_sleeps={})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tinysleepnet_env2/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# Print training, validation and test sets\n",
    "logger.info(\"Training set (n_night_sleeps={})\".format(len(train_y)))\n",
    "for _x in train_x: logger.info(_x.shape)\n",
    "print_n_samples_each_class(np.hstack(train_y))\n",
    "logger.info(\"Validation set (n_night_sleeps={})\".format(len(valid_y)))\n",
    "for _x in valid_x: logger.info(_x.shape)\n",
    "print_n_samples_each_class(np.hstack(valid_y))\n",
    "logger.info(\"Test set (n_night_sleeps={})\".format(len(test_y)))\n",
    "for _x in test_x: logger.info(_x.shape)\n",
    "print_n_samples_each_class(np.hstack(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force to use 1.5 only for N1\n",
    "if config.get('weighted_cross_ent') is None:\n",
    "    config['weighted_cross_ent'] = False\n",
    "    logger.info(f'  Weighted cross entropy: Not specified --> default: {config[\"weighted_cross_ent\"]}')\n",
    "else:\n",
    "    logger.info(f'  Weighted cross entropy: {config[\"weighted_cross_ent\"]}')\n",
    "if config['weighted_cross_ent']:\n",
    "    config[\"class_weights\"] = np.asarray([1., 1.5, 1., 1., 1.], dtype=np.float32)\n",
    "else:\n",
    "    config[\"class_weights\"] = np.asarray([1., 1., 1., 1., 1.], dtype=np.float32)\n",
    "logger.info(f'  Weighted cross entropy: {config[\"class_weights\"]}')\n",
    "\n",
    "# Create a model\n",
    "device = torch.device(\"cuda:{}\".format(args.gpu) if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f'using device {args.gpu}')\n",
    "model = Model(\n",
    "    config=config,\n",
    "    output_dir=output_dir,\n",
    "    use_rnn=True,\n",
    "    testing=False,\n",
    "    use_best=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Data Augmentation Details\n",
    "logger.info('Data Augmentation')\n",
    "logger.info(f'  Sequence: {config[\"augment_seq\"]}')\n",
    "logger.info(f'  Signal full: {config[\"augment_signal_full\"]}')\n",
    "\n",
    "# Train using epoch scheme\n",
    "best_acc = -1\n",
    "best_mf1 = -1\n",
    "update_epoch = -1\n",
    "config[\"n_epochs\"] = args.n_epochs\n",
    "for epoch in range(model.get_current_epoch(), config[\"n_epochs\"]):\n",
    "    # Create minibatches for training\n",
    "    shuffle_idx = np.random.permutation(np.arange(len(train_x)))  # shuffle every epoch is good for generalization\n",
    "    # Create augmented data\n",
    "    percent = 0.1\n",
    "    aug_train_x = np.copy(train_x)\n",
    "    aug_train_y = np.copy(train_y)\n",
    "    for i in range(len(aug_train_x)):\n",
    "        # Shift signals horizontally\n",
    "        offset = np.random.uniform(-percent, percent) * aug_train_x[i].shape[1]\n",
    "        roll_x = np.roll(aug_train_x[i], int(offset))\n",
    "        if offset < 0:\n",
    "            aug_train_x[i] = roll_x[:-1]\n",
    "            aug_train_y[i] = aug_train_y[i][:-1]\n",
    "        if offset > 0:\n",
    "            aug_train_x[i] = roll_x[1:]\n",
    "            aug_train_y[i] = aug_train_y[i][1:]\n",
    "        roll_x = None\n",
    "        assert len(aug_train_x[i]) == len(aug_train_y[i])\n",
    "    aug_minibatch_fn = iterate_batch_multiple_seq_minibatches(\n",
    "        aug_train_x,\n",
    "        aug_train_y,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        seq_length=config[\"seq_length\"],\n",
    "        shuffle_idx=shuffle_idx,\n",
    "        augment_seq=config['augment_seq'],\n",
    "    )\n",
    "    # Train, one epoch,\n",
    "    train_outs = model.train_with_dataloader(aug_minibatch_fn)  # 只使用增强后的数据进行训练， 每个epoch进行一次数据增强\n",
    "    # Create minibatches for validation\n",
    "    valid_minibatch_fn = iterate_batch_multiple_seq_minibatches(\n",
    "        valid_x,\n",
    "        valid_y,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        seq_length=config[\"seq_length\"],\n",
    "        shuffle_idx=None,\n",
    "        augment_seq=False,\n",
    "    )\n",
    "    valid_outs = model.evaluate_with_dataloader(valid_minibatch_fn)\n",
    "\n",
    "    # Create minibatches for testing\n",
    "    test_minibatch_fn = iterate_batch_multiple_seq_minibatches(\n",
    "        test_x,\n",
    "        test_y,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        seq_length=config[\"seq_length\"],\n",
    "        shuffle_idx=None,\n",
    "        augment_seq=False,\n",
    "    )\n",
    "    test_outs = model.evaluate_with_dataloader(test_minibatch_fn)\n",
    "\n",
    "    writer = model.train_writer\n",
    "    writer.add_scalar(tag=\"e_losses/train\", scalar_value=train_outs[\"train/loss\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_losses/valid\", scalar_value=valid_outs[\"test/loss\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_losses/test\", scalar_value=test_outs[\"test/loss\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_losses/epoch\", scalar_value=epoch + 1, global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_accuracy/train\", scalar_value=train_outs[\"train/accuracy\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_accuracy/valid\", scalar_value=valid_outs[\"test/accuracy\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_accuracy/test\", scalar_value=test_outs[\"test/accuracy\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_accuracy/epoch\", scalar_value=epoch + 1, global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_f1_score/train\", scalar_value=train_outs[\"train/f1_score\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_f1_score/valid\", scalar_value=valid_outs[\"test/f1_score\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_f1_score/test\", scalar_value=test_outs[\"test/f1_score\"], global_step=train_outs[\"global_step\"])\n",
    "    writer.add_scalar(tag=\"e_f1_score/epoch\", scalar_value=epoch + 1, global_step=train_outs[\"global_step\"])\n",
    "\n",
    "    logger.info(\"[e{}/{} s{}] TR (n={}) l={:.4f} a={:.1f} f1={:.1f} ({:.1f}s)| \"\n",
    "                \"VA (n={}) l={:.4f} a={:.1f}, f1={:.1f} ({:.1f}s) | \"\n",
    "                \"TE (n={}) l={:.4f} a={:.1f}, f1={:.1f} ({:.1f}s)\".format(\n",
    "        epoch+1,\n",
    "        config[\"n_epochs\"],\n",
    "        train_outs[\"global_step\"],\n",
    "        len(train_outs[\"train/trues\"]),\n",
    "        train_outs[\"train/loss\"],\n",
    "        train_outs[\"train/accuracy\"] * 100,\n",
    "        train_outs[\"train/f1_score\"] * 100,\n",
    "        train_outs[\"train/duration\"],\n",
    "\n",
    "        len(valid_outs[\"test/trues\"]),\n",
    "        valid_outs[\"test/loss\"],\n",
    "        valid_outs[\"test/accuracy\"] * 100,\n",
    "        valid_outs[\"test/f1_score\"] * 100,\n",
    "        valid_outs[\"test/duration\"],\n",
    "\n",
    "        len(test_outs[\"test/trues\"]),\n",
    "        test_outs[\"test/loss\"],\n",
    "        test_outs[\"test/accuracy\"] * 100,\n",
    "        test_outs[\"test/f1_score\"] * 100,\n",
    "        test_outs[\"test/duration\"],\n",
    "        )\n",
    "    )\n",
    "    # Check best model\n",
    "    if best_acc < valid_outs[\"test/accuracy\"] and \\\n",
    "        best_mf1 <= valid_outs[\"test/f1_score\"]:\n",
    "        best_acc = valid_outs[\"test/accuracy\"]\n",
    "        best_mf1 = valid_outs[\"test/f1_score\"]\n",
    "        update_epoch = epoch+1\n",
    "        model.save_best_checkpoint(name=\"best_model\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    if (epoch+1) % config[\"evaluate_span\"] == 0 or (epoch+1) == config[\"n_epochs\"]:\n",
    "        logger.info(\">> Confusion Matrix\")\n",
    "        logger.info(test_outs[\"test/cm\"])\n",
    "    #\n",
    "    # # Save checkpoint\n",
    "    # if (epoch+1) % config[\"checkpoint_span\"] == 0 or (epoch+1) == config[\"n_epochs\"]:\n",
    "    #     model.save_checkpoint(name=\"model\")\n",
    "    #\n",
    "    # # Early stopping\n",
    "    # if update_epoch > 0 and ((epoch+1) - update_epoch) > config[\"no_improve_epochs\"]:\n",
    "    #     logger.info(\"*** Early-stopping ***\")\n",
    "    #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
