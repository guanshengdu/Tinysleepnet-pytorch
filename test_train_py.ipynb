{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from data import load_data, get_subject_files\n",
    "from model import Model\n",
    "from minibatching import iterate_batch_multiple_seq_minibatches\n",
    "from utils import print_n_samples_each_class, load_seq_ids\n",
    "from logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    config_file=args.config_file,\n",
    "    fold_idx=args.fold_idx,\n",
    "    output_dir=args.output_dir,\n",
    "    log_file=args.log_file,\n",
    "    restart=args.restart,\n",
    "    random_seed=args.random_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(\n",
    "    args,\n",
    "    config_file,\n",
    "    fold_idx,\n",
    "    output_dir,\n",
    "    log_file,\n",
    "    restart=False,\n",
    "    random_seed=42,\n",
    "):\n",
    "    spec = importlib.util.spec_from_file_location(\"*\", config_file)\n",
    "    config = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(config)\n",
    "    config = config.train\n",
    "\n",
    "    # Create output directory for the specified fold_idx\n",
    "    output_dir = os.path.join(output_dir, str(fold_idx))\n",
    "    if restart:\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "        os.makedirs(output_dir)\n",
    "    else:\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "    # Create logger\n",
    "    logger = get_logger(log_file, level=\"info\")\n",
    "\n",
    "    subject_files = glob.glob(os.path.join(config[\"data_dir\"], \"*.npz\"))\n",
    "\n",
    "    # Load subject IDs\n",
    "    fname = \"{}.txt\".format(config[\"dataset\"])\n",
    "    seq_sids = load_seq_ids(fname)\n",
    "    logger.info(\"Load generated SIDs from {}\".format(fname))\n",
    "    logger.info(\"SIDs ({}): {}\".format(len(seq_sids), seq_sids))\n",
    "\n",
    "    # Split training and test sets\n",
    "    fold_pids = np.array_split(seq_sids, config[\"n_folds\"])\n",
    "\n",
    "    test_sids = fold_pids[fold_idx]\n",
    "    train_sids = np.setdiff1d(seq_sids, test_sids)\n",
    "\n",
    "    # Further split training set as validation set (10%)\n",
    "    n_valids = round(len(train_sids) * 0.10)\n",
    "\n",
    "    # Set random seed to control the randomness\n",
    "    np.random.seed(random_seed)\n",
    "    valid_sids = np.random.choice(train_sids, size=n_valids, replace=False)\n",
    "    train_sids = np.setdiff1d(train_sids, valid_sids)\n",
    "\n",
    "    logger.info(\"Train SIDs: ({}) {}\".format(len(train_sids), train_sids))\n",
    "    logger.info(\"Valid SIDs: ({}) {}\".format(len(valid_sids), valid_sids))\n",
    "    logger.info(\"Test SIDs: ({}) {}\".format(len(test_sids), test_sids))\n",
    "\n",
    "    # Get corresponding files\n",
    "    train_files = []\n",
    "    for sid in train_sids:\n",
    "        train_files.append(\n",
    "            get_subject_files(\n",
    "                dataset=config[\"dataset\"],\n",
    "                files=subject_files,\n",
    "                sid=sid,\n",
    "            )\n",
    "        )\n",
    "    train_files = np.hstack(train_files)\n",
    "    train_x, train_y, _ = load_data(train_files)\n",
    "\n",
    "    valid_files = []\n",
    "    for sid in valid_sids:\n",
    "        valid_files.append(\n",
    "            get_subject_files(\n",
    "                dataset=config[\"dataset\"],\n",
    "                files=subject_files,\n",
    "                sid=sid,\n",
    "            )\n",
    "        )\n",
    "    valid_files = np.hstack(valid_files)\n",
    "    valid_x, valid_y, _ = load_data(valid_files)\n",
    "\n",
    "    test_files = []\n",
    "    for sid in test_sids:\n",
    "        test_files.append(\n",
    "            get_subject_files(\n",
    "                dataset=config[\"dataset\"],\n",
    "                files=subject_files,\n",
    "                sid=sid,\n",
    "            )\n",
    "        )\n",
    "    test_files = np.hstack(test_files)\n",
    "    test_x, test_y, _ = load_data(test_files)\n",
    "\n",
    "    # Print training, validation and test sets\n",
    "    logger.info(\"Training set (n_night_sleeps={})\".format(len(train_y)))\n",
    "    for _x in train_x:\n",
    "        logger.info(_x.shape)\n",
    "    print_n_samples_each_class(np.hstack(train_y))\n",
    "    logger.info(\"Validation set (n_night_sleeps={})\".format(len(valid_y)))\n",
    "    for _x in valid_x:\n",
    "        logger.info(_x.shape)\n",
    "    print_n_samples_each_class(np.hstack(valid_y))\n",
    "    logger.info(\"Test set (n_night_sleeps={})\".format(len(test_y)))\n",
    "    for _x in test_x:\n",
    "        logger.info(_x.shape)\n",
    "    print_n_samples_each_class(np.hstack(test_y))\n",
    "\n",
    "    # Force to use 1.5 only for N1\n",
    "    if config.get(\"weighted_cross_ent\") is None:\n",
    "        config[\"weighted_cross_ent\"] = False\n",
    "        logger.info(\n",
    "            f'  Weighted cross entropy: Not specified --> default: {config[\"weighted_cross_ent\"]}'\n",
    "        )\n",
    "    else:\n",
    "        logger.info(f'  Weighted cross entropy: {config[\"weighted_cross_ent\"]}')\n",
    "    if config[\"weighted_cross_ent\"]:\n",
    "        config[\"class_weights\"] = np.asarray(\n",
    "            [1.0, 1.5, 1.0, 1.0, 1.0], dtype=np.float32\n",
    "        )\n",
    "    else:\n",
    "        config[\"class_weights\"] = np.asarray(\n",
    "            [1.0, 1.0, 1.0, 1.0, 1.0], dtype=np.float32\n",
    "        )\n",
    "    logger.info(f'  Weighted cross entropy: {config[\"class_weights\"]}')\n",
    "\n",
    "    # Create a model\n",
    "    device = torch.device(\n",
    "        \"cuda:{}\".format(args.gpu) if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    logger.info(f\"using device {args.gpu}\")\n",
    "    model = Model(\n",
    "        config=config,\n",
    "        output_dir=output_dir,\n",
    "        use_rnn=True,\n",
    "        testing=False,\n",
    "        use_best=False,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Data Augmentation Details\n",
    "    logger.info(\"Data Augmentation\")\n",
    "    logger.info(f'  Sequence: {config[\"augment_seq\"]}')\n",
    "    logger.info(f'  Signal full: {config[\"augment_signal_full\"]}')\n",
    "\n",
    "    # Train using epoch scheme\n",
    "    best_acc = -1\n",
    "    best_mf1 = -1\n",
    "    update_epoch = -1\n",
    "    config[\"n_epochs\"] = args.n_epochs\n",
    "    for epoch in range(model.get_current_epoch(), config[\"n_epochs\"]):\n",
    "        # Create minibatches for training\n",
    "        shuffle_idx = np.random.permutation(\n",
    "            np.arange(len(train_x))\n",
    "        )  # shuffle every epoch is good for generalization\n",
    "        # Create augmented data\n",
    "        percent = 0.1\n",
    "        aug_train_x = np.copy(train_x)\n",
    "        aug_train_y = np.copy(train_y)\n",
    "        for i in range(len(aug_train_x)):\n",
    "            # Shift signals horizontally\n",
    "            offset = np.random.uniform(-percent, percent) * aug_train_x[i].shape[1]\n",
    "            roll_x = np.roll(aug_train_x[i], int(offset))\n",
    "            if offset < 0:\n",
    "                aug_train_x[i] = roll_x[:-1]\n",
    "                aug_train_y[i] = aug_train_y[i][:-1]\n",
    "            if offset > 0:\n",
    "                aug_train_x[i] = roll_x[1:]\n",
    "                aug_train_y[i] = aug_train_y[i][1:]\n",
    "            roll_x = None\n",
    "            assert len(aug_train_x[i]) == len(aug_train_y[i])\n",
    "        aug_minibatch_fn = iterate_batch_multiple_seq_minibatches(\n",
    "            aug_train_x,\n",
    "            aug_train_y,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            seq_length=config[\"seq_length\"],\n",
    "            shuffle_idx=shuffle_idx,\n",
    "            augment_seq=config[\"augment_seq\"],\n",
    "        )\n",
    "        # Train, one epoch,\n",
    "        train_outs = model.train_with_dataloader(\n",
    "            aug_minibatch_fn\n",
    "        )  # 只使用增强后的数据进行训练， 每个epoch进行一次数据增强\n",
    "        # Create minibatches for validation\n",
    "        valid_minibatch_fn = iterate_batch_multiple_seq_minibatches(\n",
    "            valid_x,\n",
    "            valid_y,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            seq_length=config[\"seq_length\"],\n",
    "            shuffle_idx=None,\n",
    "            augment_seq=False,\n",
    "        )\n",
    "        valid_outs = model.evaluate_with_dataloader(valid_minibatch_fn)\n",
    "\n",
    "        # Create minibatches for testing\n",
    "        test_minibatch_fn = iterate_batch_multiple_seq_minibatches(\n",
    "            test_x,\n",
    "            test_y,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            seq_length=config[\"seq_length\"],\n",
    "            shuffle_idx=None,\n",
    "            augment_seq=False,\n",
    "        )\n",
    "        test_outs = model.evaluate_with_dataloader(test_minibatch_fn)\n",
    "\n",
    "        writer = model.train_writer\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_losses/train\",\n",
    "            scalar_value=train_outs[\"train/loss\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_losses/valid\",\n",
    "            scalar_value=valid_outs[\"test/loss\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_losses/test\",\n",
    "            scalar_value=test_outs[\"test/loss\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_losses/epoch\",\n",
    "            scalar_value=epoch + 1,\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_accuracy/train\",\n",
    "            scalar_value=train_outs[\"train/accuracy\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_accuracy/valid\",\n",
    "            scalar_value=valid_outs[\"test/accuracy\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_accuracy/test\",\n",
    "            scalar_value=test_outs[\"test/accuracy\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_accuracy/epoch\",\n",
    "            scalar_value=epoch + 1,\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_f1_score/train\",\n",
    "            scalar_value=train_outs[\"train/f1_score\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_f1_score/valid\",\n",
    "            scalar_value=valid_outs[\"test/f1_score\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_f1_score/test\",\n",
    "            scalar_value=test_outs[\"test/f1_score\"],\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"e_f1_score/epoch\",\n",
    "            scalar_value=epoch + 1,\n",
    "            global_step=train_outs[\"global_step\"],\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            \"[e{}/{} s{}] TR (n={}) l={:.4f} a={:.1f} f1={:.1f} ({:.1f}s)| \"\n",
    "            \"VA (n={}) l={:.4f} a={:.1f}, f1={:.1f} ({:.1f}s) | \"\n",
    "            \"TE (n={}) l={:.4f} a={:.1f}, f1={:.1f} ({:.1f}s)\".format(\n",
    "                epoch + 1,\n",
    "                config[\"n_epochs\"],\n",
    "                train_outs[\"global_step\"],\n",
    "                len(train_outs[\"train/trues\"]),\n",
    "                train_outs[\"train/loss\"],\n",
    "                train_outs[\"train/accuracy\"] * 100,\n",
    "                train_outs[\"train/f1_score\"] * 100,\n",
    "                train_outs[\"train/duration\"],\n",
    "                len(valid_outs[\"test/trues\"]),\n",
    "                valid_outs[\"test/loss\"],\n",
    "                valid_outs[\"test/accuracy\"] * 100,\n",
    "                valid_outs[\"test/f1_score\"] * 100,\n",
    "                valid_outs[\"test/duration\"],\n",
    "                len(test_outs[\"test/trues\"]),\n",
    "                test_outs[\"test/loss\"],\n",
    "                test_outs[\"test/accuracy\"] * 100,\n",
    "                test_outs[\"test/f1_score\"] * 100,\n",
    "                test_outs[\"test/duration\"],\n",
    "            )\n",
    "        )\n",
    "        # Check best model\n",
    "        if (\n",
    "            best_acc < valid_outs[\"test/accuracy\"]\n",
    "            and best_mf1 <= valid_outs[\"test/f1_score\"]\n",
    "        ):\n",
    "            best_acc = valid_outs[\"test/accuracy\"]\n",
    "            best_mf1 = valid_outs[\"test/f1_score\"]\n",
    "            update_epoch = epoch + 1\n",
    "            model.save_best_checkpoint(name=\"best_model\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        if (epoch + 1) % config[\"evaluate_span\"] == 0 or (epoch + 1) == config[\n",
    "            \"n_epochs\"\n",
    "        ]:\n",
    "            logger.info(\">> Confusion Matrix\")\n",
    "            logger.info(test_outs[\"test/cm\"])\n",
    "        #\n",
    "        # # Save checkpoint\n",
    "        # if (epoch+1) % config[\"checkpoint_span\"] == 0 or (epoch+1) == config[\"n_epochs\"]:\n",
    "        #     model.save_checkpoint(name=\"model\")\n",
    "        #\n",
    "        # # Early stopping\n",
    "        # if update_epoch > 0 and ((epoch+1) - update_epoch) > config[\"no_improve_epochs\"]:\n",
    "        #     logger.info(\"*** Early-stopping ***\")\n",
    "        #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinysleepnet_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
